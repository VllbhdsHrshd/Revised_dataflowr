{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b8553b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:31.642239Z",
     "iopub.status.busy": "2023-11-01T12:31:31.641637Z",
     "iopub.status.idle": "2023-11-01T12:31:35.239875Z",
     "shell.execute_reply": "2023-11-01T12:31:35.238545Z"
    },
    "papermill": {
     "duration": 3.612319,
     "end_time": "2023-11-01T12:31:35.243144",
     "exception": false,
     "start_time": "2023-11-01T12:31:31.630825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here in this notebook we're going to work on the linear regression\n",
    "## Using numpy[First principle], torch's tensor, neural network and then with an inbuilt pytorch module\n",
    "\n",
    "## so lets get started.....\n",
    "import numpy as np\n",
    "import torch as tr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89d9c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.262516Z",
     "iopub.status.busy": "2023-11-01T12:31:35.261973Z",
     "iopub.status.idle": "2023-11-01T12:31:35.267759Z",
     "shell.execute_reply": "2023-11-01T12:31:35.266512Z"
    },
    "papermill": {
     "duration": 0.018747,
     "end_time": "2023-11-01T12:31:35.270338",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.251591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## So the base equation that we'll be working with will be\n",
    "## y_t = 2*x_t - 3*x_t**2 + 1, for over 30 samples\n",
    "\n",
    "from numpy.random import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd2dc7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.288651Z",
     "iopub.status.busy": "2023-11-01T12:31:35.288258Z",
     "iopub.status.idle": "2023-11-01T12:31:35.300113Z",
     "shell.execute_reply": "2023-11-01T12:31:35.299150Z"
    },
    "papermill": {
     "duration": 0.023819,
     "end_time": "2023-11-01T12:31:35.302320",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.278501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52541219, 0.50299945],\n",
       "       [0.98215495, 0.02098079],\n",
       "       [0.59093436, 0.78980699],\n",
       "       [0.83408857, 0.50300441],\n",
       "       [0.78223817, 0.07051406]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate random data\n",
    "x = random((30,2))\n",
    "## generate labels corresponding to input data x\n",
    "y = np.dot(x,[2., -3.,]) + 1\n",
    "w_source = np.array([2., -3.])\n",
    "b_source = np.array([1.])\n",
    "\n",
    "x[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16afc78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.321575Z",
     "iopub.status.busy": "2023-11-01T12:31:35.320598Z",
     "iopub.status.idle": "2023-11-01T12:31:35.331523Z",
     "shell.execute_reply": "2023-11-01T12:31:35.330376Z"
    },
    "papermill": {
     "duration": 0.024368,
     "end_time": "2023-11-01T12:31:35.335095",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.310727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_figs(fig_num, elev, azim, x, y, weights, bias):\n",
    "    fig = plt.figure(fig_num, figsize=(4,3))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, elev=elev, azim=azim)\n",
    "    ax.scatter(x[:,0],x[:,1], y)\n",
    "    ax.plot_surface(np.array([[0,0],[1,1]]), \n",
    "                   np.array([[0,1],[0,1]]),\n",
    "                    (np.dot(np.array([[0,0,1,1],[0,1,0,1]]).T, weights) + bias).reshape(2,2), alpha=0.5)\n",
    "    ax.set_xlabel('x_1')\n",
    "    ax.set_ylabel('x_2')\n",
    "    ax.set_zlabel('y')\n",
    "\n",
    "def plot_views(x,y,w,b):\n",
    "    elev = 43.5\n",
    "    azim = -110\n",
    "    plot_figs(1,elev, azim, x, y, w,b[0])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7471db62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.354374Z",
     "iopub.status.busy": "2023-11-01T12:31:35.353927Z",
     "iopub.status.idle": "2023-11-01T12:31:35.412441Z",
     "shell.execute_reply": "2023-11-01T12:31:35.410698Z"
    },
    "papermill": {
     "duration": 0.073209,
     "end_time": "2023-11-01T12:31:35.417434",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.344225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 400x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_views(x,y, w_source, b_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f673fb4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.457740Z",
     "iopub.status.busy": "2023-11-01T12:31:35.456369Z",
     "iopub.status.idle": "2023-11-01T12:31:35.467860Z",
     "shell.execute_reply": "2023-11-01T12:31:35.466335Z"
    },
    "papermill": {
     "duration": 0.036339,
     "end_time": "2023-11-01T12:31:35.472036",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.435697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Values of the Parameters:  [0.45424486 0.72050176] [0.65662812]\n"
     ]
    }
   ],
   "source": [
    "w_init = random(2)\n",
    "b_init = random(1)\n",
    "\n",
    "w = w_init\n",
    "b = b_init\n",
    "print(\"Initial Values of the Parameters: \", w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daff4635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.512251Z",
     "iopub.status.busy": "2023-11-01T12:31:35.510875Z",
     "iopub.status.idle": "2023-11-01T12:31:35.635313Z",
     "shell.execute_reply": "2023-11-01T12:31:35.634170Z"
    },
    "papermill": {
     "duration": 0.148177,
     "end_time": "2023-11-01T12:31:35.638236",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.490059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss::  55.44930233162283\n",
      "Progress::  epoch::  0 loss 55.44930233162283\n",
      "Progress::  epoch::  1 loss 41.74773264909031\n",
      "Progress::  epoch::  2 loss 36.38628814576933\n",
      "Progress::  epoch::  3 loss 31.899432402164276\n",
      "Progress::  epoch::  4 loss 27.991779310488575\n",
      "Progress::  epoch::  5 loss 24.58311765705068\n",
      "Progress::  epoch::  6 loss 21.608707915949473\n",
      "Progress::  epoch::  7 loss 19.012372319331345\n",
      "Progress::  epoch::  8 loss 16.74524038881043\n",
      "Progress::  epoch::  9 loss 14.764793481642629\n",
      "Progress::  epoch::  10 loss 13.034037674785948\n",
      "Progress::  epoch::  11 loss 11.520784509294991\n",
      "Progress::  epoch::  12 loss 10.19702543426205\n",
      "Progress::  epoch::  13 loss 9.038387738735992\n",
      "Progress::  epoch::  14 loss 8.023661354214022\n",
      "Progress::  epoch::  15 loss 7.134387293856239\n",
      "Progress::  epoch::  16 loss 6.3544996977611214\n",
      "Progress::  epoch::  17 loss 5.670014500045032\n",
      "Progress::  epoch::  18 loss 5.06875864352215\n",
      "Progress::  epoch::  19 loss 4.540134559250464\n",
      "Progress::  epoch::  20 loss 4.0749153165467815\n",
      "Progress::  epoch::  21 loss 3.665066447717977\n",
      "Progress::  epoch::  22 loss 3.303590972394128\n",
      "Progress::  epoch::  23 loss 2.984394599146595\n",
      "Progress::  epoch::  24 loss 2.7021684758704683\n",
      "Progress::  epoch::  25 loss 2.4522872028942846\n",
      "Progress::  epoch::  26 loss 2.230720120636799\n",
      "Progress::  epoch::  27 loss 2.0339541426761505\n",
      "Progress::  epoch::  28 loss 1.858926630387982\n",
      "Progress::  epoch::  29 loss 1.7029670012446396\n",
      "Progress::  epoch::  30 loss 1.5637459332724917\n",
      "Progress::  epoch::  31 loss 1.4392311763654027\n",
      "Progress::  epoch::  32 loss 1.327649110042929\n",
      "Progress::  epoch::  33 loss 1.2274512993379032\n",
      "Progress::  epoch::  34 loss 1.1372853979882358\n",
      "Progress::  epoch::  35 loss 1.0559698328951697\n",
      "Progress::  epoch::  36 loss 0.9824717775501458\n",
      "Progress::  epoch::  37 loss 0.9158879862642351\n",
      "Progress::  epoch::  38 loss 0.855428116809957\n",
      "Progress::  epoch::  39 loss 0.8004002175938733\n",
      "Progress::  epoch::  40 loss 0.750198097666601\n",
      "Progress::  epoch::  41 loss 0.7042903345684343\n",
      "Progress::  epoch::  42 loss 0.662210706919749\n",
      "Progress::  epoch::  43 loss 0.6235498664187779\n",
      "Progress::  epoch::  44 loss 0.5879480880468315\n",
      "Progress::  epoch::  45 loss 0.5550889582738701\n",
      "Progress::  epoch::  46 loss 0.5246938793151137\n",
      "Progress::  epoch::  47 loss 0.4965172833686225\n",
      "Progress::  epoch::  48 loss 0.47034246457428014\n",
      "Progress::  epoch::  49 loss 0.4459779484460334\n",
      "Progress::  epoch::  50 loss 0.4232543289758848\n",
      "Progress::  epoch::  51 loss 0.40202151269398245\n",
      "Progress::  epoch::  52 loss 0.3821463168714255\n",
      "Progress::  epoch::  53 loss 0.3635103759253868\n",
      "Progress::  epoch::  54 loss 0.3460083160638652\n",
      "Progress::  epoch::  55 loss 0.32954616340657045\n",
      "Progress::  epoch::  56 loss 0.314039955340451\n",
      "Progress::  epoch::  57 loss 0.2994145288014698\n",
      "Progress::  epoch::  58 loss 0.28560246259514704\n",
      "Progress::  epoch::  59 loss 0.2725431538438273\n",
      "Progress::  epoch::  60 loss 0.26018201123663814\n",
      "Progress::  epoch::  61 loss 0.24846975000915805\n",
      "Progress::  epoch::  62 loss 0.2373617755378128\n",
      "Progress::  epoch::  63 loss 0.22681764413710778\n",
      "Progress::  epoch::  64 loss 0.21680059112925165\n",
      "Progress::  epoch::  65 loss 0.20727711754432274\n",
      "Progress::  epoch::  66 loss 0.19821662793007663\n",
      "Progress::  epoch::  67 loss 0.18959111272555462\n",
      "Progress::  epoch::  68 loss 0.18137486950088877\n",
      "Progress::  epoch::  69 loss 0.17354425810358565\n",
      "Estimation of the parameters::  [ 2.19585646 -2.72628773] [0.79191923]\n"
     ]
    }
   ],
   "source": [
    "## Lets define our functions\n",
    "## Forward function\n",
    "def forward(x):\n",
    "    return x.dot(w)+b\n",
    "## loss functions\n",
    "def loss(x,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y - y_pred)**2\n",
    "\n",
    "print(\"Initial loss:: \", np.sum([loss(x_val, y_val) for x_val, y_val in zip(x,y)]))\n",
    "\n",
    "\n",
    "## compute gradients\n",
    "def gradient(x,y):\n",
    "    return 2*(x.dot(w)+b - y)*x, 2*(x.dot(w)+b - y) \n",
    "\n",
    "learning_rate = 1e-2\n",
    "## Training loop\n",
    "\n",
    "\n",
    "for epoch in range(70):\n",
    "    grad_w = np.array([0,0])\n",
    "    grad_b = np.array(0)\n",
    "    l = 0\n",
    "    for x_val, y_val in zip(x,y):\n",
    "        grad_w = np.add(grad_w, gradient(x_val, y_val)[0])\n",
    "        grad_b = np.add(grad_b, gradient(x_val, y_val)[1])\n",
    "        l += loss(x_val, y_val)\n",
    "    \n",
    "    w = w - learning_rate*grad_w\n",
    "    b = b - learning_rate*grad_b\n",
    "    print(\"Progress:: \",\"epoch:: \",epoch,\"loss\",l[0])\n",
    "\n",
    "## After Training\n",
    "print(\"Estimation of the parameters:: \", w,b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bfb77",
   "metadata": {
    "papermill": {
     "duration": 0.008447,
     "end_time": "2023-11-01T12:31:35.655483",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.647036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Lets do the same with Pytorch Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484d2587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.674769Z",
     "iopub.status.busy": "2023-11-01T12:31:35.674389Z",
     "iopub.status.idle": "2023-11-01T12:31:35.728624Z",
     "shell.execute_reply": "2023-11-01T12:31:35.727293Z"
    },
    "papermill": {
     "duration": 0.067238,
     "end_time": "2023-11-01T12:31:35.731370",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.664132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Linear Regression with tensors\n",
    "dtype = tr.FloatTensor\n",
    "x_t = tr.from_numpy(x).type(dtype)\n",
    "y_t = tr.from_numpy(y).type(dtype).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4729e9e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.750989Z",
     "iopub.status.busy": "2023-11-01T12:31:35.750535Z",
     "iopub.status.idle": "2023-11-01T12:31:35.817861Z",
     "shell.execute_reply": "2023-11-01T12:31:35.816491Z"
    },
    "papermill": {
     "duration": 0.080524,
     "end_time": "2023-11-01T12:31:35.820817",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.740293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5254, 0.5030],\n",
      "        [0.9822, 0.0210],\n",
      "        [0.5909, 0.7898],\n",
      "        [0.8341, 0.5030],\n",
      "        [0.7822, 0.0705],\n",
      "        [0.3508, 0.0503],\n",
      "        [0.5284, 0.0654],\n",
      "        [0.2686, 0.6132],\n",
      "        [0.4547, 0.5046],\n",
      "        [0.9771, 0.0229],\n",
      "        [0.0915, 0.5195],\n",
      "        [0.9731, 0.1173],\n",
      "        [0.9066, 0.2760],\n",
      "        [0.5495, 0.8158],\n",
      "        [0.4425, 0.4475],\n",
      "        [0.0387, 0.0710],\n",
      "        [0.4897, 0.5664],\n",
      "        [0.0101, 0.7234],\n",
      "        [0.3933, 0.8477],\n",
      "        [0.6492, 0.3869],\n",
      "        [0.1898, 0.4523],\n",
      "        [0.1010, 0.4918],\n",
      "        [0.1009, 0.8102],\n",
      "        [0.4745, 0.3107],\n",
      "        [0.1932, 0.6181],\n",
      "        [0.0079, 0.2781],\n",
      "        [0.2715, 0.4195],\n",
      "        [0.6589, 0.2593],\n",
      "        [0.2409, 0.8576],\n",
      "        [0.3370, 0.0107]]) tensor([[ 0.5418],\n",
      "        [ 2.9014],\n",
      "        [-0.1876],\n",
      "        [ 1.1592],\n",
      "        [ 2.3529],\n",
      "        [ 1.5507],\n",
      "        [ 1.8605],\n",
      "        [-0.3025],\n",
      "        [ 0.3957],\n",
      "        [ 2.8855],\n",
      "        [-0.3755],\n",
      "        [ 2.5942],\n",
      "        [ 1.9854],\n",
      "        [-0.3485],\n",
      "        [ 0.5425],\n",
      "        [ 0.8645],\n",
      "        [ 0.2804],\n",
      "        [-1.1498],\n",
      "        [-0.7565],\n",
      "        [ 1.1377],\n",
      "        [ 0.0227],\n",
      "        [-0.2736],\n",
      "        [-1.2288],\n",
      "        [ 1.0167],\n",
      "        [-0.4680],\n",
      "        [ 0.1816],\n",
      "        [ 0.2846],\n",
      "        [ 1.5399],\n",
      "        [-1.0909],\n",
      "        [ 1.6419]])\n"
     ]
    }
   ],
   "source": [
    "print(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6f79f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.840251Z",
     "iopub.status.busy": "2023-11-01T12:31:35.839837Z",
     "iopub.status.idle": "2023-11-01T12:31:35.849529Z",
     "shell.execute_reply": "2023-11-01T12:31:35.848327Z"
    },
    "papermill": {
     "duration": 0.022221,
     "end_time": "2023-11-01T12:31:35.851884",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.829663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Values of the parameters :  tensor([[0.4542],\n",
      "        [0.7205]]) tensor([[0.6566]])\n"
     ]
    }
   ],
   "source": [
    "w_init_t = tr.from_numpy(w_init).type(dtype)\n",
    "b_init_t = tr.from_numpy(b_init).type(dtype)\n",
    "\n",
    "# print(w_init_t, b_init_t)\n",
    "w_t = w_init_t.clone()\n",
    "# print(w_t, w_t.ndim)\n",
    "w_t.unsqueeze_(1)\n",
    "# print(w_t, w_t.ndim)\n",
    "b_t = b_init_t.clone()\n",
    "# print(b_t)\n",
    "b_t.unsqueeze_(1)\n",
    "print(\"Initial Values of the parameters : \", w_t, b_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ee2c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.872000Z",
     "iopub.status.busy": "2023-11-01T12:31:35.871517Z",
     "iopub.status.idle": "2023-11-01T12:31:35.928591Z",
     "shell.execute_reply": "2023-11-01T12:31:35.927081Z"
    },
    "papermill": {
     "duration": 0.070643,
     "end_time": "2023-11-01T12:31:35.931878",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.861235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  Epoch:  0 __ Loss:  tensor(55.4493)\n",
      "Progress:  Epoch:  1 __ Loss:  tensor(41.7477)\n",
      "Progress:  Epoch:  2 __ Loss:  tensor(36.3863)\n",
      "Progress:  Epoch:  3 __ Loss:  tensor(31.8994)\n",
      "Progress:  Epoch:  4 __ Loss:  tensor(27.9918)\n",
      "Progress:  Epoch:  5 __ Loss:  tensor(24.5831)\n",
      "Progress:  Epoch:  6 __ Loss:  tensor(21.6087)\n",
      "Progress:  Epoch:  7 __ Loss:  tensor(19.0124)\n",
      "Progress:  Epoch:  8 __ Loss:  tensor(16.7452)\n",
      "Progress:  Epoch:  9 __ Loss:  tensor(14.7648)\n",
      "Progress:  Epoch:  10 __ Loss:  tensor(13.0340)\n",
      "Progress:  Epoch:  11 __ Loss:  tensor(11.5208)\n",
      "Progress:  Epoch:  12 __ Loss:  tensor(10.1970)\n",
      "Progress:  Epoch:  13 __ Loss:  tensor(9.0384)\n",
      "Progress:  Epoch:  14 __ Loss:  tensor(8.0237)\n",
      "Progress:  Epoch:  15 __ Loss:  tensor(7.1344)\n",
      "Progress:  Epoch:  16 __ Loss:  tensor(6.3545)\n",
      "Progress:  Epoch:  17 __ Loss:  tensor(5.6700)\n",
      "Progress:  Epoch:  18 __ Loss:  tensor(5.0688)\n",
      "Progress:  Epoch:  19 __ Loss:  tensor(4.5401)\n",
      "Progress:  Epoch:  20 __ Loss:  tensor(4.0749)\n",
      "Progress:  Epoch:  21 __ Loss:  tensor(3.6651)\n",
      "Progress:  Epoch:  22 __ Loss:  tensor(3.3036)\n",
      "Progress:  Epoch:  23 __ Loss:  tensor(2.9844)\n",
      "Progress:  Epoch:  24 __ Loss:  tensor(2.7022)\n",
      "Progress:  Epoch:  25 __ Loss:  tensor(2.4523)\n",
      "Progress:  Epoch:  26 __ Loss:  tensor(2.2307)\n",
      "Progress:  Epoch:  27 __ Loss:  tensor(2.0340)\n",
      "Progress:  Epoch:  28 __ Loss:  tensor(1.8589)\n",
      "Progress:  Epoch:  29 __ Loss:  tensor(1.7030)\n",
      "Progress:  Epoch:  30 __ Loss:  tensor(1.5637)\n",
      "Progress:  Epoch:  31 __ Loss:  tensor(1.4392)\n",
      "Progress:  Epoch:  32 __ Loss:  tensor(1.3276)\n",
      "Progress:  Epoch:  33 __ Loss:  tensor(1.2275)\n",
      "Progress:  Epoch:  34 __ Loss:  tensor(1.1373)\n",
      "Progress:  Epoch:  35 __ Loss:  tensor(1.0560)\n",
      "Progress:  Epoch:  36 __ Loss:  tensor(0.9825)\n",
      "Progress:  Epoch:  37 __ Loss:  tensor(0.9159)\n",
      "Progress:  Epoch:  38 __ Loss:  tensor(0.8554)\n",
      "Progress:  Epoch:  39 __ Loss:  tensor(0.8004)\n",
      "Progress:  Epoch:  40 __ Loss:  tensor(0.7502)\n",
      "Progress:  Epoch:  41 __ Loss:  tensor(0.7043)\n",
      "Progress:  Epoch:  42 __ Loss:  tensor(0.6622)\n",
      "Progress:  Epoch:  43 __ Loss:  tensor(0.6235)\n",
      "Progress:  Epoch:  44 __ Loss:  tensor(0.5879)\n",
      "Progress:  Epoch:  45 __ Loss:  tensor(0.5551)\n",
      "Progress:  Epoch:  46 __ Loss:  tensor(0.5247)\n",
      "Progress:  Epoch:  47 __ Loss:  tensor(0.4965)\n",
      "Progress:  Epoch:  48 __ Loss:  tensor(0.4703)\n",
      "Progress:  Epoch:  49 __ Loss:  tensor(0.4460)\n",
      "Progress:  Epoch:  50 __ Loss:  tensor(0.4233)\n",
      "Progress:  Epoch:  51 __ Loss:  tensor(0.4020)\n",
      "Progress:  Epoch:  52 __ Loss:  tensor(0.3821)\n",
      "Progress:  Epoch:  53 __ Loss:  tensor(0.3635)\n",
      "Progress:  Epoch:  54 __ Loss:  tensor(0.3460)\n",
      "Progress:  Epoch:  55 __ Loss:  tensor(0.3295)\n",
      "Progress:  Epoch:  56 __ Loss:  tensor(0.3140)\n",
      "Progress:  Epoch:  57 __ Loss:  tensor(0.2994)\n",
      "Progress:  Epoch:  58 __ Loss:  tensor(0.2856)\n",
      "Progress:  Epoch:  59 __ Loss:  tensor(0.2725)\n",
      "Progress:  Epoch:  60 __ Loss:  tensor(0.2602)\n",
      "Progress:  Epoch:  61 __ Loss:  tensor(0.2485)\n",
      "Progress:  Epoch:  62 __ Loss:  tensor(0.2374)\n",
      "Progress:  Epoch:  63 __ Loss:  tensor(0.2268)\n",
      "Progress:  Epoch:  64 __ Loss:  tensor(0.2168)\n",
      "Progress:  Epoch:  65 __ Loss:  tensor(0.2073)\n",
      "Progress:  Epoch:  66 __ Loss:  tensor(0.1982)\n",
      "Progress:  Epoch:  67 __ Loss:  tensor(0.1896)\n",
      "Progress:  Epoch:  68 __ Loss:  tensor(0.1814)\n",
      "Progress:  Epoch:  69 __ Loss:  tensor(0.1735)\n",
      "estimation of the parameters::  tensor([[ 2.1959],\n",
      "        [-2.7263]]) tensor([[0.7919]])\n"
     ]
    }
   ],
   "source": [
    "## Forward Pass\n",
    "def forward_t(x):\n",
    "    return x.mm(w_t)+b_t\n",
    "## Loss Function\n",
    "def loss_t(x,y):\n",
    "    y_pred = forward_t(x)\n",
    "    return (y_pred - y).pow(2).sum()\n",
    "## compute gradient\n",
    "def gradient_t(x,y):\n",
    "    return 2*tr.mm(tr.t(x), x.mm(w_t)+b_t - y), 2*(x.mm(w_t)+b_t - y).sum()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for epoch in range(70):\n",
    "    l_t = loss_t(x_t, y_t)\n",
    "    grad_w, grad_b = gradient_t(x_t, y_t)\n",
    "    w_t = w_t - learning_rate * grad_w\n",
    "    b_t = b_t - learning_rate * grad_b\n",
    "    print(\"Progress: \",\"Epoch: \", epoch,\"__\",\"Loss: \",l_t)\n",
    "\n",
    "print(\"estimation of the parameters:: \", w_t, b_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433c820",
   "metadata": {
    "papermill": {
     "duration": 0.008804,
     "end_time": "2023-11-01T12:31:35.949868",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.941064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Linear Regression with AutoGrad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7238798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:35.970222Z",
     "iopub.status.busy": "2023-11-01T12:31:35.969822Z",
     "iopub.status.idle": "2023-11-01T12:31:35.978222Z",
     "shell.execute_reply": "2023-11-01T12:31:35.976789Z"
    },
    "papermill": {
     "duration": 0.021703,
     "end_time": "2023-11-01T12:31:35.980634",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.958931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial values of the parameters:  tensor([[0.4542],\n",
      "        [0.7205]]) tensor([[0.6566]])\n"
     ]
    }
   ],
   "source": [
    "w_v = w_init_t.clone().unsqueeze(1)\n",
    "w_v.requires_grad_(True)\n",
    "b_v = b_init_t.clone().unsqueeze(1)\n",
    "b_v.requires_grad_(True)\n",
    "print(\"initial values of the parameters: \", w_v.data, b_v.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54220525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:36.002206Z",
     "iopub.status.busy": "2023-11-01T12:31:36.001279Z",
     "iopub.status.idle": "2023-11-01T12:31:36.046820Z",
     "shell.execute_reply": "2023-11-01T12:31:36.044687Z"
    },
    "papermill": {
     "duration": 0.059251,
     "end_time": "2023-11-01T12:31:36.049773",
     "exception": false,
     "start_time": "2023-11-01T12:31:35.990522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  epoch:  loss:  55.44930648803711\n",
      "Progress:  epoch:  loss:  41.74773025512695\n",
      "Progress:  epoch:  loss:  36.386287689208984\n",
      "Progress:  epoch:  loss:  31.89943504333496\n",
      "Progress:  epoch:  loss:  27.991777420043945\n",
      "Progress:  epoch:  loss:  24.583118438720703\n",
      "Progress:  epoch:  loss:  21.60870933532715\n",
      "Progress:  epoch:  loss:  19.012374877929688\n",
      "Progress:  epoch:  loss:  16.7452392578125\n",
      "Progress:  epoch:  loss:  14.764793395996094\n",
      "Progress:  epoch:  loss:  13.034037590026855\n",
      "Progress:  epoch:  loss:  11.520784378051758\n",
      "Progress:  epoch:  loss:  10.197025299072266\n",
      "Progress:  epoch:  loss:  9.038389205932617\n",
      "Progress:  epoch:  loss:  8.023661613464355\n",
      "Progress:  epoch:  loss:  7.134387493133545\n",
      "Progress:  epoch:  loss:  6.354499816894531\n",
      "Progress:  epoch:  loss:  5.670013904571533\n",
      "Progress:  epoch:  loss:  5.068758964538574\n",
      "Progress:  epoch:  loss:  4.540134906768799\n",
      "Progress:  epoch:  loss:  4.074915409088135\n",
      "Progress:  epoch:  loss:  3.665066957473755\n",
      "Progress:  epoch:  loss:  3.303591251373291\n",
      "Progress:  epoch:  loss:  2.9843947887420654\n",
      "Progress:  epoch:  loss:  2.7021687030792236\n",
      "Progress:  epoch:  loss:  2.452286958694458\n",
      "Progress:  epoch:  loss:  2.230719804763794\n",
      "Progress:  epoch:  loss:  2.033953905105591\n",
      "Progress:  epoch:  loss:  1.858926773071289\n",
      "Progress:  epoch:  loss:  1.7029669284820557\n",
      "Progress:  epoch:  loss:  1.5637460947036743\n",
      "Progress:  epoch:  loss:  1.439231038093567\n",
      "Progress:  epoch:  loss:  1.3276491165161133\n",
      "Progress:  epoch:  loss:  1.227450966835022\n",
      "Progress:  epoch:  loss:  1.1372852325439453\n",
      "Progress:  epoch:  loss:  1.0559697151184082\n",
      "Progress:  epoch:  loss:  0.9824719429016113\n",
      "Progress:  epoch:  loss:  0.9158884286880493\n",
      "Progress:  epoch:  loss:  0.8554283976554871\n",
      "Progress:  epoch:  loss:  0.8004001379013062\n",
      "Progress:  epoch:  loss:  0.7501978278160095\n",
      "Progress:  epoch:  loss:  0.704289972782135\n",
      "Progress:  epoch:  loss:  0.6622103452682495\n",
      "Progress:  epoch:  loss:  0.6235491633415222\n",
      "Progress:  epoch:  loss:  0.5879475474357605\n",
      "Progress:  epoch:  loss:  0.5550885200500488\n",
      "Progress:  epoch:  loss:  0.5246933698654175\n",
      "Progress:  epoch:  loss:  0.49651673436164856\n",
      "Progress:  epoch:  loss:  0.47034212946891785\n",
      "Progress:  epoch:  loss:  0.44597747921943665\n",
      "Progress:  epoch:  loss:  0.42325374484062195\n",
      "Progress:  epoch:  loss:  0.4020208716392517\n",
      "Progress:  epoch:  loss:  0.3821457028388977\n",
      "Progress:  epoch:  loss:  0.36350977420806885\n",
      "Progress:  epoch:  loss:  0.34600770473480225\n",
      "Progress:  epoch:  loss:  0.3295457661151886\n",
      "Progress:  epoch:  loss:  0.3140396177768707\n",
      "Progress:  epoch:  loss:  0.2994139790534973\n",
      "Progress:  epoch:  loss:  0.285601943731308\n",
      "Progress:  epoch:  loss:  0.2725427746772766\n",
      "Progress:  epoch:  loss:  0.26018139719963074\n",
      "Progress:  epoch:  loss:  0.24846935272216797\n",
      "Progress:  epoch:  loss:  0.23736147582530975\n",
      "Progress:  epoch:  loss:  0.22681722044944763\n",
      "Progress:  epoch:  loss:  0.2168000489473343\n",
      "Progress:  epoch:  loss:  0.20727649331092834\n",
      "Progress:  epoch:  loss:  0.19821611046791077\n",
      "Progress:  epoch:  loss:  0.1895906925201416\n",
      "Progress:  epoch:  loss:  0.18137426674365997\n",
      "Progress:  epoch:  loss:  0.17354364693164825\n",
      "estimation of the parameters:  tensor([[ 2.1959],\n",
      "        [-2.7263]]) tensor([[0.7919]])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(70):\n",
    "    y_pred = x_t.mm(w_v)+b_v\n",
    "    loss = (y_pred - y_t).pow(2).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with tr.no_grad():\n",
    "        w_v -= learning_rate * w_v.grad\n",
    "        b_v -= learning_rate * b_v.grad\n",
    "    \n",
    "    w_v.grad.zero_()\n",
    "    b_v.grad.zero_()\n",
    "    \n",
    "    print(\"Progress: \", \"epoch: \", \"loss: \", loss.data.item())\n",
    "\n",
    "print(\"estimation of the parameters: \", w_v.data, b_v.data.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc70cf5",
   "metadata": {
    "papermill": {
     "duration": 0.009382,
     "end_time": "2023-11-01T12:31:36.068628",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.059246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Linear Regression with Neural Network with Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692e3bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:36.089630Z",
     "iopub.status.busy": "2023-11-01T12:31:36.089002Z",
     "iopub.status.idle": "2023-11-01T12:31:36.135542Z",
     "shell.execute_reply": "2023-11-01T12:31:36.133309Z"
    },
    "papermill": {
     "duration": 0.060183,
     "end_time": "2023-11-01T12:31:36.138344",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.078161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress :  epoch:  0 loss:  55.44930648803711\n",
      "Progress :  epoch:  1 loss:  41.74773025512695\n",
      "Progress :  epoch:  2 loss:  36.386287689208984\n",
      "Progress :  epoch:  3 loss:  31.89943504333496\n",
      "Progress :  epoch:  4 loss:  27.991777420043945\n",
      "Progress :  epoch:  5 loss:  24.583118438720703\n",
      "Progress :  epoch:  6 loss:  21.60870933532715\n",
      "Progress :  epoch:  7 loss:  19.012374877929688\n",
      "Progress :  epoch:  8 loss:  16.7452392578125\n",
      "Progress :  epoch:  9 loss:  14.764793395996094\n",
      "Progress :  epoch:  10 loss:  13.034037590026855\n",
      "Progress :  epoch:  11 loss:  11.520784378051758\n",
      "Progress :  epoch:  12 loss:  10.197025299072266\n",
      "Progress :  epoch:  13 loss:  9.038389205932617\n",
      "Progress :  epoch:  14 loss:  8.023661613464355\n",
      "Progress :  epoch:  15 loss:  7.134387493133545\n",
      "Progress :  epoch:  16 loss:  6.354499816894531\n",
      "Progress :  epoch:  17 loss:  5.670013904571533\n",
      "Progress :  epoch:  18 loss:  5.068758964538574\n",
      "Progress :  epoch:  19 loss:  4.540134906768799\n",
      "Progress :  epoch:  20 loss:  4.074915409088135\n",
      "Progress :  epoch:  21 loss:  3.665066957473755\n",
      "Progress :  epoch:  22 loss:  3.303591251373291\n",
      "Progress :  epoch:  23 loss:  2.9843947887420654\n",
      "Progress :  epoch:  24 loss:  2.7021687030792236\n",
      "Progress :  epoch:  25 loss:  2.452286958694458\n",
      "Progress :  epoch:  26 loss:  2.230719804763794\n",
      "Progress :  epoch:  27 loss:  2.033953905105591\n",
      "Progress :  epoch:  28 loss:  1.858926773071289\n",
      "Progress :  epoch:  29 loss:  1.7029669284820557\n",
      "Progress :  epoch:  30 loss:  1.5637460947036743\n",
      "Progress :  epoch:  31 loss:  1.439231038093567\n",
      "Progress :  epoch:  32 loss:  1.3276491165161133\n",
      "Progress :  epoch:  33 loss:  1.227450966835022\n",
      "Progress :  epoch:  34 loss:  1.1372852325439453\n",
      "Progress :  epoch:  35 loss:  1.0559697151184082\n",
      "Progress :  epoch:  36 loss:  0.9824719429016113\n",
      "Progress :  epoch:  37 loss:  0.9158884286880493\n",
      "Progress :  epoch:  38 loss:  0.8554283976554871\n",
      "Progress :  epoch:  39 loss:  0.8004001379013062\n",
      "Progress :  epoch:  40 loss:  0.7501978278160095\n",
      "Progress :  epoch:  41 loss:  0.704289972782135\n",
      "Progress :  epoch:  42 loss:  0.6622103452682495\n",
      "Progress :  epoch:  43 loss:  0.6235491633415222\n",
      "Progress :  epoch:  44 loss:  0.5879475474357605\n",
      "Progress :  epoch:  45 loss:  0.5550885200500488\n",
      "Progress :  epoch:  46 loss:  0.5246933698654175\n",
      "Progress :  epoch:  47 loss:  0.49651673436164856\n",
      "Progress :  epoch:  48 loss:  0.47034212946891785\n",
      "Progress :  epoch:  49 loss:  0.44597747921943665\n",
      "Progress :  epoch:  50 loss:  0.42325374484062195\n",
      "Progress :  epoch:  51 loss:  0.4020208716392517\n",
      "Progress :  epoch:  52 loss:  0.3821457028388977\n",
      "Progress :  epoch:  53 loss:  0.36350977420806885\n",
      "Progress :  epoch:  54 loss:  0.34600770473480225\n",
      "Progress :  epoch:  55 loss:  0.3295457661151886\n",
      "Progress :  epoch:  56 loss:  0.3140396177768707\n",
      "Progress :  epoch:  57 loss:  0.2994139790534973\n",
      "Progress :  epoch:  58 loss:  0.285601943731308\n",
      "Progress :  epoch:  59 loss:  0.2725427746772766\n",
      "Progress :  epoch:  60 loss:  0.26018139719963074\n",
      "Progress :  epoch:  61 loss:  0.24846935272216797\n",
      "Progress :  epoch:  62 loss:  0.23736147582530975\n",
      "Progress :  epoch:  63 loss:  0.22681722044944763\n",
      "Progress :  epoch:  64 loss:  0.2168000489473343\n",
      "Progress :  epoch:  65 loss:  0.20727649331092834\n",
      "Progress :  epoch:  66 loss:  0.19821611046791077\n",
      "Progress :  epoch:  67 loss:  0.1895906925201416\n",
      "Progress :  epoch:  68 loss:  0.18137426674365997\n",
      "Progress :  epoch:  69 loss:  0.17354364693164825\n",
      "estimation of the parameter:  \n",
      "Parameter containing:\n",
      "tensor([[ 2.1959, -2.7263]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.7919], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = tr.nn.Sequential(\n",
    "tr.nn.Linear(2,1),\n",
    "    \n",
    ")\n",
    "\n",
    "for m in model.children():\n",
    "    m.weight.data = w_init_t.clone().unsqueeze(0)\n",
    "    m.bias.data = b_init_t.clone()\n",
    "\n",
    "loss_fn = tr.nn.MSELoss(reduction='sum')\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(70):\n",
    "    y_pred = model(x_t)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y_t)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with tr.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "    \n",
    "    print(\"Progress : \", \"epoch: \", epoch, \"loss: \", loss.data.item())\n",
    "\n",
    "print(\"estimation of the parameter:  \")\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b04f8",
   "metadata": {
    "papermill": {
     "duration": 0.009726,
     "end_time": "2023-11-01T12:31:36.157880",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.148154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b225d9ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:36.179335Z",
     "iopub.status.busy": "2023-11-01T12:31:36.178404Z",
     "iopub.status.idle": "2023-11-01T12:31:36.211956Z",
     "shell.execute_reply": "2023-11-01T12:31:36.210562Z"
    },
    "papermill": {
     "duration": 0.047536,
     "end_time": "2023-11-01T12:31:36.214994",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.167458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  epoch:  0 loss :  55.44930648803711\n",
      "Progress:  epoch:  1 loss :  41.74773025512695\n",
      "Progress:  epoch:  2 loss :  36.386287689208984\n",
      "Progress:  epoch:  3 loss :  31.89943504333496\n",
      "Progress:  epoch:  4 loss :  27.991777420043945\n",
      "Progress:  epoch:  5 loss :  24.583118438720703\n",
      "Progress:  epoch:  6 loss :  21.60870933532715\n",
      "Progress:  epoch:  7 loss :  19.012372970581055\n",
      "Progress:  epoch:  8 loss :  16.7452392578125\n",
      "Progress:  epoch:  9 loss :  14.764793395996094\n",
      "Progress:  epoch:  10 loss :  13.034037590026855\n",
      "Progress:  epoch:  11 loss :  11.520784378051758\n",
      "Progress:  epoch:  12 loss :  10.197025299072266\n",
      "Progress:  epoch:  13 loss :  9.038389205932617\n",
      "Progress:  epoch:  14 loss :  8.023661613464355\n",
      "Progress:  epoch:  15 loss :  7.134387493133545\n",
      "Progress:  epoch:  16 loss :  6.354499816894531\n",
      "Progress:  epoch:  17 loss :  5.670013904571533\n",
      "Progress:  epoch:  18 loss :  5.068758964538574\n",
      "Progress:  epoch:  19 loss :  4.540134906768799\n",
      "Progress:  epoch:  20 loss :  4.074915409088135\n",
      "Progress:  epoch:  21 loss :  3.6650660037994385\n",
      "Progress:  epoch:  22 loss :  3.303591012954712\n",
      "Progress:  epoch:  23 loss :  2.9843943119049072\n",
      "Progress:  epoch:  24 loss :  2.7021682262420654\n",
      "Progress:  epoch:  25 loss :  2.452286720275879\n",
      "Progress:  epoch:  26 loss :  2.2307193279266357\n",
      "Progress:  epoch:  27 loss :  2.0339529514312744\n",
      "Progress:  epoch:  28 loss :  1.8589259386062622\n",
      "Progress:  epoch:  29 loss :  1.7029662132263184\n",
      "Progress:  epoch:  30 loss :  1.563745141029358\n",
      "Progress:  epoch:  31 loss :  1.43923020362854\n",
      "Progress:  epoch:  32 loss :  1.3276479244232178\n",
      "Progress:  epoch:  33 loss :  1.2274501323699951\n",
      "Progress:  epoch:  34 loss :  1.137284517288208\n",
      "Progress:  epoch:  35 loss :  1.0559691190719604\n",
      "Progress:  epoch:  36 loss :  0.9824714660644531\n",
      "Progress:  epoch:  37 loss :  0.9158878326416016\n",
      "Progress:  epoch:  38 loss :  0.855427622795105\n",
      "Progress:  epoch:  39 loss :  0.800399661064148\n",
      "Progress:  epoch:  40 loss :  0.7501974105834961\n",
      "Progress:  epoch:  41 loss :  0.704289436340332\n",
      "Progress:  epoch:  42 loss :  0.6622098088264465\n",
      "Progress:  epoch:  43 loss :  0.6235492825508118\n",
      "Progress:  epoch:  44 loss :  0.5879477262496948\n",
      "Progress:  epoch:  45 loss :  0.5550885796546936\n",
      "Progress:  epoch:  46 loss :  0.5246934294700623\n",
      "Progress:  epoch:  47 loss :  0.4965170621871948\n",
      "Progress:  epoch:  48 loss :  0.4703422486782074\n",
      "Progress:  epoch:  49 loss :  0.44597768783569336\n",
      "Progress:  epoch:  50 loss :  0.4232538938522339\n",
      "Progress:  epoch:  51 loss :  0.40202096104621887\n",
      "Progress:  epoch:  52 loss :  0.38214582204818726\n",
      "Progress:  epoch:  53 loss :  0.3635098934173584\n",
      "Progress:  epoch:  54 loss :  0.3460078835487366\n",
      "Progress:  epoch:  55 loss :  0.3295457065105438\n",
      "Progress:  epoch:  56 loss :  0.3140396177768707\n",
      "Progress:  epoch:  57 loss :  0.2994140386581421\n",
      "Progress:  epoch:  58 loss :  0.285601943731308\n",
      "Progress:  epoch:  59 loss :  0.2725427746772766\n",
      "Progress:  epoch:  60 loss :  0.26018139719963074\n",
      "Progress:  epoch:  61 loss :  0.24846935272216797\n",
      "Progress:  epoch:  62 loss :  0.23736147582530975\n",
      "Progress:  epoch:  63 loss :  0.22681722044944763\n",
      "Progress:  epoch:  64 loss :  0.2168000489473343\n",
      "Progress:  epoch:  65 loss :  0.20727649331092834\n",
      "Progress:  epoch:  66 loss :  0.19821611046791077\n",
      "Progress:  epoch:  67 loss :  0.1895906925201416\n",
      "Progress:  epoch:  68 loss :  0.18137426674365997\n",
      "Progress:  epoch:  69 loss :  0.17354364693164825\n",
      "Estimation of the parameters :  \n",
      "Parameter containing:\n",
      "tensor([[ 2.1959, -2.7263]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.7919], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = tr.nn.Sequential(\n",
    "tr.nn.Linear(2,1))\n",
    "for m in model.children():\n",
    "    m.weight.data = w_init_t.clone().unsqueeze(0)\n",
    "    m.bias.data = b_init_t.clone()\n",
    "loss_fn = tr.nn.MSELoss(reduction='sum')\n",
    "model.train()\n",
    "\n",
    "optimizer = tr.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(70):\n",
    "    y_pred=model(x_t)\n",
    "#     model.zero_grad()\n",
    "    loss = loss_fn(y_pred , y_t)\n",
    "    \n",
    "    print(\"Progress: \", \"epoch: \", epoch, \"loss : \", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(\"Estimation of the parameters :  \")\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae8cf8",
   "metadata": {
    "papermill": {
     "duration": 0.009682,
     "end_time": "2023-11-01T12:31:36.234525",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.224843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Now its time to use the inbuilt pytorch function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834a8321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:36.255723Z",
     "iopub.status.busy": "2023-11-01T12:31:36.255348Z",
     "iopub.status.idle": "2023-11-01T12:31:36.299812Z",
     "shell.execute_reply": "2023-11-01T12:31:36.298486Z"
    },
    "papermill": {
     "duration": 0.058827,
     "end_time": "2023-11-01T12:31:36.303077",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.244250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0000],\n",
       "        [-3.0000],\n",
       "        [ 1.0000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb_t = tr.cat((x_t, tr.ones(30).unsqueeze(1)),1)\n",
    "sol = tr.linalg.lstsq(xb_t, y_t)\n",
    "sol.solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c848a",
   "metadata": {
    "papermill": {
     "duration": 0.009612,
     "end_time": "2023-11-01T12:31:36.323236",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.313624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**One more Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0f03b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:36.344999Z",
     "iopub.status.busy": "2023-11-01T12:31:36.344556Z",
     "iopub.status.idle": "2023-11-01T12:31:36.351735Z",
     "shell.execute_reply": "2023-11-01T12:31:36.350528Z"
    },
    "papermill": {
     "duration": 0.021117,
     "end_time": "2023-11-01T12:31:36.354194",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.333077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = random((300,2))\n",
    "y = np.dot(x, [2.,-3.,]) + 1\n",
    "x_t = tr.from_numpy(x).type(dtype)\n",
    "y_t = tr.from_numpy(y).type(dtype).unsqueeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e22ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T12:31:36.376176Z",
     "iopub.status.busy": "2023-11-01T12:31:36.375696Z",
     "iopub.status.idle": "2023-11-01T12:31:36.418734Z",
     "shell.execute_reply": "2023-11-01T12:31:36.417493Z"
    },
    "papermill": {
     "duration": 0.057819,
     "end_time": "2023-11-01T12:31:36.421990",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.364171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  Epoch:  0 loss 518.9248657226562\n",
      "Progress:  Epoch:  1 loss 10265.5107421875\n",
      "Progress:  Epoch:  2 loss 662972.5625\n",
      "Progress:  Epoch:  3 loss 43262568.0\n",
      "Progress:  Epoch:  4 loss 2823255296.0\n",
      "Progress:  Epoch:  5 loss 184241717248.0\n",
      "Progress:  Epoch:  6 loss 12023360389120.0\n",
      "Progress:  Epoch:  7 loss 784627912409088.0\n",
      "Progress:  Epoch:  8 loss 5.12037368144855e+16\n",
      "Progress:  Epoch:  9 loss 3.341484831165907e+18\n",
      "Progress:  Epoch:  10 loss 2.1806070515132636e+20\n",
      "Progress:  Epoch:  11 loss 1.423034012047638e+22\n",
      "Progress:  Epoch:  12 loss 9.286524753421497e+23\n",
      "Progress:  Epoch:  13 loss 6.0602590243268e+25\n",
      "Progress:  Epoch:  14 loss 3.9548411438525573e+27\n",
      "Progress:  Epoch:  15 loss 2.5808750971096318e+29\n",
      "Progress:  Epoch:  16 loss 1.6842432944602996e+31\n",
      "Progress:  Epoch:  17 loss 1.0991140389448097e+33\n",
      "Progress:  Epoch:  18 loss 7.172666182404813e+34\n",
      "Progress:  Epoch:  19 loss 4.680782728059634e+36\n",
      "Progress:  Epoch:  20 loss 3.0546144498375086e+38\n",
      "Progress:  Epoch:  21 loss inf\n",
      "Progress:  Epoch:  22 loss inf\n",
      "Progress:  Epoch:  23 loss inf\n",
      "Progress:  Epoch:  24 loss inf\n",
      "Progress:  Epoch:  25 loss inf\n",
      "Progress:  Epoch:  26 loss inf\n",
      "Progress:  Epoch:  27 loss inf\n",
      "Progress:  Epoch:  28 loss inf\n",
      "Progress:  Epoch:  29 loss inf\n",
      "Progress:  Epoch:  30 loss inf\n",
      "Progress:  Epoch:  31 loss inf\n",
      "Progress:  Epoch:  32 loss inf\n",
      "Progress:  Epoch:  33 loss inf\n",
      "Progress:  Epoch:  34 loss inf\n",
      "Progress:  Epoch:  35 loss inf\n",
      "Progress:  Epoch:  36 loss inf\n",
      "Progress:  Epoch:  37 loss inf\n",
      "Progress:  Epoch:  38 loss inf\n",
      "Progress:  Epoch:  39 loss inf\n",
      "Progress:  Epoch:  40 loss inf\n",
      "Progress:  Epoch:  41 loss inf\n",
      "Progress:  Epoch:  42 loss nan\n",
      "Progress:  Epoch:  43 loss nan\n",
      "Progress:  Epoch:  44 loss nan\n",
      "Progress:  Epoch:  45 loss nan\n",
      "Progress:  Epoch:  46 loss nan\n",
      "Progress:  Epoch:  47 loss nan\n",
      "Progress:  Epoch:  48 loss nan\n",
      "Progress:  Epoch:  49 loss nan\n",
      "Progress:  Epoch:  50 loss nan\n",
      "Progress:  Epoch:  51 loss nan\n",
      "Progress:  Epoch:  52 loss nan\n",
      "Progress:  Epoch:  53 loss nan\n",
      "Progress:  Epoch:  54 loss nan\n",
      "Progress:  Epoch:  55 loss nan\n",
      "Progress:  Epoch:  56 loss nan\n",
      "Progress:  Epoch:  57 loss nan\n",
      "Progress:  Epoch:  58 loss nan\n",
      "Progress:  Epoch:  59 loss nan\n",
      "Progress:  Epoch:  60 loss nan\n",
      "Progress:  Epoch:  61 loss nan\n",
      "Progress:  Epoch:  62 loss nan\n",
      "Progress:  Epoch:  63 loss nan\n",
      "Progress:  Epoch:  64 loss nan\n",
      "Progress:  Epoch:  65 loss nan\n",
      "Progress:  Epoch:  66 loss nan\n",
      "Progress:  Epoch:  67 loss nan\n",
      "Progress:  Epoch:  68 loss nan\n",
      "Progress:  Epoch:  69 loss nan\n",
      "Estimation of the parameter: \n",
      "Parameter containing:\n",
      "tensor([[nan, nan]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = tr.nn.Sequential(\n",
    "tr.nn.Linear(2,1))\n",
    "\n",
    "for m in model.children():\n",
    "    m.weight.data = w_init_t.clone().unsqueeze(0)\n",
    "    m.bias.data = b_init_t.clone()\n",
    "loss_fn = tr.nn.MSELoss(reduction='sum')\n",
    "model.train()\n",
    "optimizer = tr.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(70):\n",
    "    y_pred = model(x_t)\n",
    "    loss = loss_fn(y_pred , y_t)\n",
    "    \n",
    "    print(\"Progress: \", \"Epoch: \", epoch, \"loss\", loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Estimation of the parameter: \")\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c1b3c",
   "metadata": {
    "papermill": {
     "duration": 0.010067,
     "end_time": "2023-11-01T12:31:36.442452",
     "exception": false,
     "start_time": "2023-11-01T12:31:36.432385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.486351,
   "end_time": "2023-11-01T12:31:39.453848",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T12:31:27.967497",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
